{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to load data from a JSON file\n",
    "def load_json_data(json_filepath: str):\n",
    "    \"\"\"\n",
    "    Loads data from a JSON file and returns it as a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        json_filepath (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Paths to the JSON files for DIMAURO and MOLINETTE datasets\n",
    "dimauro_json_filepath = r'/Users/benedettaperrone/Documents/TESI/VISUAL_STUDIO_CODE/whisper/DIMAURO/DIMAURO_new.json'\n",
    "molinette_json_filepath = r'/Users/benedettaperrone/Documents/TESI/VISUAL_STUDIO_CODE/whisper/MOLINETTE_combined/MOLINETTE_combined.json'\n",
    "\n",
    "# Load JSON data\n",
    "dimauro_data = load_json_data(dimauro_json_filepath)\n",
    "molinette_data = load_json_data(molinette_json_filepath)\n",
    "\n",
    "# Function to extract the patient ID from the filename in the DIMAURO dataset\n",
    "def extract_id_from_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the patient ID from the filename based on specific patterns.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to extract the ID from.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted patient ID.\n",
    "    \"\"\"\n",
    "    if filename.startswith(('B1', 'B2')):\n",
    "        patient_id = filename[2:].split('.')[0]\n",
    "    elif filename.startswith(('FB1', 'PR1')):\n",
    "        patient_id = filename[3:].split('.')[0]\n",
    "    else:\n",
    "        patient_id = ''\n",
    "\n",
    "    # Logic to extract ID based on the presence of two digits followed by 'F' or 'M'\n",
    "    age_found = False\n",
    "    for i in range(len(patient_id)):\n",
    "        if patient_id[i].isdigit() and not age_found:\n",
    "            if i + 1 < len(patient_id) and patient_id[i+1].isdigit():\n",
    "                age_found = True\n",
    "        elif age_found and patient_id[i] in ['F', 'M']:\n",
    "            patient_id = patient_id[:i+1]\n",
    "            break\n",
    "    return patient_id\n",
    "\n",
    "def extract_id_from_molinette_filename(filename: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Extracts the patient ID and on_off status from the MOLINETTE filename based on the pattern after the first 'P'.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to extract the ID and on_off status from.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (patient_id (str), on_off (str or None))\n",
    "    \"\"\"\n",
    "    patient_id = ''\n",
    "    on_off = None\n",
    "    \n",
    "    if filename.startswith('P'):\n",
    "        # Extract ID for filenames that start with 'P'\n",
    "        p_index = filename.find('P') + 1  # Get the index just after 'P'\n",
    "        while p_index < len(filename) and filename[p_index].isdigit():\n",
    "            patient_id += filename[p_index]\n",
    "            p_index += 1\n",
    "        if 'ON' in filename:\n",
    "            on_off = 'on'\n",
    "        elif 'OFF' in filename:\n",
    "            on_off = 'off'\n",
    "    elif filename.startswith('OP'):\n",
    "        # Extract ID for filenames that start with 'OP'\n",
    "        patient_id = filename[:5]  # Take 'OP' + first 3 digits as ID\n",
    "        on_off = 'on'  # Files with 'OP' are assumed to be 'on'\n",
    "        \n",
    "    return patient_id, on_off\n",
    "\n",
    "\n",
    "# Function to extract the ID and on_off status based on the dataset\n",
    "def extract_id(filename: str, dataset: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Determines the correct ID extraction function based on the dataset.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to extract the ID from.\n",
    "        dataset (str): The dataset name ('DIMAURO' or 'MOLINETTE').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (file_id (str), file_onoff (str or None))\n",
    "    \"\"\"\n",
    "    if dataset == 'DIMAURO':\n",
    "        file_id = extract_id_from_filename(filename)\n",
    "        return file_id, None  # file_onoff is not present in DIMAURO\n",
    "    elif dataset == 'MOLINETTE':\n",
    "        file_id, file_onoff = extract_id_from_molinette_filename(filename)\n",
    "        return file_id, file_onoff\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not recognized. Use 'DIMAURO' or 'MOLINETTE'.\")\n",
    "\n",
    "# Function to determine the group, task, and UPDRS based on file ID and dataset\n",
    "def determine_group_and_task(file_id: str, json_data, dataset: str, file_onoff: str = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Determines the group (control or parkinson), task, and UPDRS associated with a file.\n",
    "\n",
    "    Args:\n",
    "        file_id (str): The extracted file ID.\n",
    "        json_data (dict): The loaded JSON data for the dataset.\n",
    "        dataset (str): The dataset name ('DIMAURO' or 'MOLINETTE').\n",
    "        file_onoff (str, optional): The on_off status for MOLINETTE dataset. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (group (str), task (str), updrs (int or None))\n",
    "    \"\"\"\n",
    "    for entry in json_data:\n",
    "        if dataset == 'MOLINETTE':\n",
    "            if entry['id'] == file_id and entry.get('on_off') == file_onoff:\n",
    "                task = entry['task'].replace(' ', '').lower()\n",
    "                if task == 'proverb':\n",
    "                    group = 'parkinson'  # MOLINETTE dataset has only parkinson patients\n",
    "                    updrs = entry.get('updrs', None)\n",
    "                    return group, task, updrs\n",
    "        elif dataset == 'DIMAURO':\n",
    "            if entry['id'] == file_id:\n",
    "                task = entry['task'].replace(' ', '').lower()\n",
    "                if task in ['readtext', 'phrases', 'words']:\n",
    "                    group = 'control' if entry['label'] == 0 else 'parkinson'\n",
    "                    updrs = entry.get('UPDRS', None)\n",
    "                    return group, task, updrs\n",
    "    return 'unknown', 'unknown', None\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the content of a text file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        str: Content of the file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "# Function to determine the reference text for MOLINETTE based on the filename\n",
    "def get_reference_text_molinette(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Determines the correct reference text for a MOLINETTE file based on its name.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to determine the reference text for.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The reference text or None if not applicable.\n",
    "    \"\"\"\n",
    "    if 'PR1a' in filename or 'PR1b' in filename:\n",
    "        return 'A caval donato non si guarda in bocca'\n",
    "    elif 'PR2a' in filename or 'PR2b' in filename:\n",
    "        return 'Meglio soli che mal accompagnati'\n",
    "    return None\n",
    "\n",
    "# Function to calculate WER and CER for control and parkinson groups\n",
    "def calculate_wer_cer_for_groups(directory: str, json_data, reference_texts: dict, dataset: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates WER and CER for control and parkinson groups based on reference texts.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing text files.\n",
    "        json_data (dict): Loaded JSON data for the dataset.\n",
    "        reference_texts (dict): Reference texts for the tasks.\n",
    "        dataset (str): Dataset name ('DIMAURO' or 'MOLINETTE').\n",
    "\n",
    "    Returns:\n",
    "        dict: Calculated WER and CER statistics.\n",
    "    \"\"\"\n",
    "    # Lists to store WER and CER values\n",
    "    control_wer = []\n",
    "    parkinsonian_wer = []\n",
    "    control_cer = []\n",
    "    parkinsonian_cer = []\n",
    "\n",
    "    # Dictionaries to store WER and CER by UPDRS level\n",
    "    parkinsonian_wer_by_updrs = {i: [] for i in range(5)}  # Considering UPDRS 0-4\n",
    "    parkinsonian_cer_by_updrs = {i: [] for i in range(5)}\n",
    "\n",
    "    # List of all .txt files in the directory\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "    # Counters for the number of files in each group and UPDRS level\n",
    "    control_files_count = 0\n",
    "    parkinsonian_files_count = 0\n",
    "    updrs_files_count = {i: 0 for i in range(5)}\n",
    "\n",
    "    for file in files:\n",
    "        # Extract file_id and file_onoff (if applicable)\n",
    "        file_id, file_onoff = extract_id(file, dataset)\n",
    "        if file_id is None:\n",
    "            continue\n",
    "\n",
    "        # Determine group, task, and UPDRS\n",
    "        group, task, updrs = determine_group_and_task(file_id, json_data, dataset, file_onoff)\n",
    "        if group == 'unknown' or task == 'unknown':\n",
    "            continue\n",
    "\n",
    "        # Get the reference text based on the dataset and task\n",
    "        if dataset == 'MOLINETTE':\n",
    "            reference = get_reference_text_molinette(file)\n",
    "        else:\n",
    "            reference = reference_texts.get(task, None)\n",
    "\n",
    "        if reference is None:\n",
    "            continue\n",
    "\n",
    "        # Read the hypothesis text from the file\n",
    "        file_path = os.path.join(directory, file)\n",
    "        hypothesis = read_file(file_path)\n",
    "\n",
    "        # Calculate WER and CER\n",
    "        wer_value = wer(reference, hypothesis)\n",
    "        cer_value = cer(reference, hypothesis)\n",
    "\n",
    "        # Assign WER and CER to the appropriate group and UPDRS level\n",
    "        if group == 'control':\n",
    "            control_wer.append(wer_value)\n",
    "            control_cer.append(cer_value)\n",
    "            control_files_count += 1\n",
    "        elif group == 'parkinson':\n",
    "            parkinsonian_wer.append(wer_value)\n",
    "            parkinsonian_cer.append(cer_value)\n",
    "            parkinsonian_files_count += 1\n",
    "            if updrs is not None and updrs in updrs_files_count:\n",
    "                parkinsonian_wer_by_updrs[updrs].append(wer_value)\n",
    "                parkinsonian_cer_by_updrs[updrs].append(cer_value)\n",
    "                updrs_files_count[updrs] += 1\n",
    "\n",
    "    # Calculate average WER and CER for control and parkinson groups\n",
    "    avg_control_wer = sum(control_wer) / len(control_wer) if control_wer else float('inf')\n",
    "    avg_parkinsonian_wer = sum(parkinsonian_wer) / len(parkinsonian_wer) if parkinsonian_wer else float('inf')\n",
    "    avg_control_cer = sum(control_cer) / len(control_cer) if control_cer else float('inf')\n",
    "    avg_parkinsonian_cer = sum(parkinsonian_cer) / len(parkinsonian_cer) if parkinsonian_cer else float('inf')\n",
    "    median_control_wer = sorted(control_wer)[len(control_wer) // 2] if control_wer else float('inf')\n",
    "    median_control_cer = sorted(control_cer)[len(control_cer) // 2] if control_cer else float('inf')\n",
    "    median_parkinsonian_wer = sorted(parkinsonian_wer)[len(parkinsonian_wer) // 2] if parkinsonian_wer else float('inf')\n",
    "    median_parkinsonian_cer = sorted(parkinsonian_cer)[len(parkinsonian_cer) // 2] if parkinsonian_cer else float('inf')\n",
    "    \n",
    "    median_parkinsonian_wer_by_updrs = {\n",
    "        i: sorted(parkinsonian_wer_by_updrs[i])[len(parkinsonian_wer_by_updrs[i]) // 2] if parkinsonian_wer_by_updrs[i] else float('inf')\n",
    "        for i in range(5)\n",
    "    }\n",
    "    \n",
    "    median_parkinsonian_cer_by_updrs = {\n",
    "        i: sorted(parkinsonian_cer_by_updrs[i])[len(parkinsonian_cer_by_updrs[i]) // 2] if parkinsonian_cer_by_updrs[i] else float('inf')\n",
    "        for i in range(5)\n",
    "    }\n",
    "    \n",
    "    # Calculate average WER and CER by UPDRS level\n",
    "    avg_parkinsonian_wer_by_updrs = {\n",
    "        i: (sum(parkinsonian_wer_by_updrs[i]) / len(parkinsonian_wer_by_updrs[i])) if parkinsonian_wer_by_updrs[i] else float('inf') \n",
    "        for i in range(5)\n",
    "    }\n",
    "    avg_parkinsonian_cer_by_updrs = {\n",
    "        i: (sum(parkinsonian_cer_by_updrs[i]) / len(parkinsonian_cer_by_updrs[i])) if parkinsonian_cer_by_updrs[i] else float('inf') \n",
    "        for i in range(5)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'control_wer': avg_control_wer,\n",
    "        'parkinsonian_wer': avg_parkinsonian_wer,\n",
    "        'control_cer': avg_control_cer,\n",
    "        'parkinsonian_cer': avg_parkinsonian_cer,\n",
    "        'control_files_count': control_files_count,\n",
    "        'parkinsonian_files_count': parkinsonian_files_count,\n",
    "        'updrs_files_count': updrs_files_count,\n",
    "        'parkinsonian_wer_by_updrs': avg_parkinsonian_wer_by_updrs,\n",
    "        'parkinsonian_cer_by_updrs': avg_parkinsonian_cer_by_updrs,\n",
    "        'individual_parkinsonian_wer': parkinsonian_wer_by_updrs,\n",
    "        'individual_parkinsonian_cer': parkinsonian_cer_by_updrs,\n",
    "        'median_control_wer': median_control_wer,\n",
    "        'median_control_cer': median_control_cer,\n",
    "        'median_parkinsonian_wer': median_parkinsonian_wer,\n",
    "        'median_parkinsonian_cer': median_parkinsonian_cer,\n",
    "        'median_parkinsonian_wer_by_updrs': median_parkinsonian_wer_by_updrs,\n",
    "        'median_parkinsonian_cer_by_updrs': median_parkinsonian_cer_by_updrs\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "from statistics import mean, median, stdev\n",
    "\n",
    "\n",
    "from statistics import mean, median, stdev\n",
    "\n",
    "# Function to integrate the results from DIMAURO and MOLINETTE datasets\n",
    "def integrate_results(dimauro_results: dict, molinette_results: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Integrates WER and CER results from DIMAURO and MOLINETTE datasets by UPDRS levels,\n",
    "    calculating both unweighted and weighted averages.\n",
    "\n",
    "    Args:\n",
    "        dimauro_results (dict): WER and CER statistics from DIMAURO dataset.\n",
    "        molinette_results (dict): WER and CER statistics from MOLINETTE dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Combined WER and CER statistics with both unweighted and weighted averages.\n",
    "    \"\"\"\n",
    "    combined_stats = {\n",
    "        'combined_wer': {},\n",
    "        'combined_cer': {},\n",
    "        'total_files_by_updrs': {}\n",
    "    }\n",
    "\n",
    "    # Only consider UPDRS levels 0, 1, and 2\n",
    "    updrs_levels = {0, 1, 2}\n",
    "\n",
    "    for i in updrs_levels:\n",
    "        # Number of files for DIMAURO and MOLINETTE for UPDRS level i\n",
    "        dimauro_count = dimauro_results['updrs_files_count'].get(i, 0)\n",
    "        molinette_count = molinette_results['updrs_files_count'].get(i, 0)\n",
    "        total_files = dimauro_count + molinette_count\n",
    "        combined_stats['total_files_by_updrs'][i] = total_files\n",
    "\n",
    "        print(f\"\\nUPDRS Level {i}\")\n",
    "        print(f\"DIMAURO file count: {dimauro_count}, MOLINETTE file count: {molinette_count}, Total files: {total_files}\")\n",
    "\n",
    "        # Retrieve individual WER and CER values for each UPDRS level\n",
    "        dimauro_wer_values = dimauro_results['individual_parkinsonian_wer'].get(i, [])\n",
    "        molinette_wer_values = molinette_results['individual_parkinsonian_wer'].get(i, [])\n",
    "        dimauro_cer_values = dimauro_results['individual_parkinsonian_cer'].get(i, [])\n",
    "        molinette_cer_values = molinette_results['individual_parkinsonian_cer'].get(i, [])\n",
    "\n",
    "        # Combine individual WER and CER values from both datasets\n",
    "        combined_wer_values = dimauro_wer_values + molinette_wer_values\n",
    "        combined_cer_values = dimauro_cer_values + molinette_cer_values\n",
    "        print(f\"Combined WER values: {combined_wer_values}\")\n",
    "        print(f\"Combined CER values: {combined_cer_values}\")\n",
    "\n",
    "        if total_files > 0:\n",
    "            # Unweighted Average (Simple Mean) Calculation\n",
    "            combined_wer_unweighted = mean(combined_wer_values) if combined_wer_values else float('inf')\n",
    "            combined_cer_unweighted = mean(combined_cer_values) if combined_cer_values else float('inf')\n",
    "            print(f\"Unweighted WER: {combined_wer_unweighted}, Unweighted CER: {combined_cer_unweighted}\")\n",
    "\n",
    "            # Weighted Average Calculation\n",
    "            # Sum of weighted WER and CER values\n",
    "            combined_wer_weighted = (\n",
    "                sum(w * dimauro_count for w in dimauro_wer_values) + sum(w * molinette_count for w in molinette_wer_values)\n",
    "            ) / (len(dimauro_wer_values) * dimauro_count + len(molinette_wer_values) * molinette_count) if combined_wer_values else float('inf')\n",
    "\n",
    "            combined_cer_weighted = (\n",
    "                sum(c * dimauro_count for c in dimauro_cer_values) + sum(c * molinette_count for c in molinette_cer_values)\n",
    "            ) / (len(dimauro_cer_values) * dimauro_count + len(molinette_cer_values) * molinette_count) if combined_cer_values else float('inf')\n",
    "\n",
    "            print(f\"Weighted WER: {combined_wer_weighted}, Weighted CER: {combined_cer_weighted}\")\n",
    "\n",
    "            # Store results in combined_stats dictionary, including the list of individual values\n",
    "            combined_stats['combined_wer'][i] = {\n",
    "                'mean_unweighted': combined_wer_unweighted,\n",
    "                'mean_weighted': combined_wer_weighted,\n",
    "                'median': median(combined_wer_values) if combined_wer_values else float('inf'),\n",
    "                'stdev': stdev(combined_wer_values) if len(combined_wer_values) > 1 else 0,\n",
    "                'values': combined_wer_values  # Store raw WER values\n",
    "            }\n",
    "\n",
    "            combined_stats['combined_cer'][i] = {\n",
    "                'mean_unweighted': combined_cer_unweighted,\n",
    "                'mean_weighted': combined_cer_weighted,\n",
    "                'median': median(combined_cer_values) if combined_cer_values else float('inf'),\n",
    "                'stdev': stdev(combined_cer_values) if len(combined_cer_values) > 1 else 0,\n",
    "                'values': combined_cer_values  # Store raw CER values\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            # If no files for this UPDRS level, set to infinity\n",
    "            combined_stats['combined_wer'][i] = {\n",
    "                'mean_unweighted': float('inf'), 'mean_weighted': float('inf'),\n",
    "                'median': float('inf'), 'stdev': float('inf'), 'values': []\n",
    "            }\n",
    "            combined_stats['combined_cer'][i] = {\n",
    "                'mean_unweighted': float('inf'), 'mean_weighted': float('inf'),\n",
    "                'median': float('inf'), 'stdev': float('inf'), 'values': []\n",
    "            }\n",
    "\n",
    "    return combined_stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths and reference texts for the DIMAURO dataset\n",
    "dimauro_directory = r\"/Users/benedettaperrone/Documents/TESI/VISUAL_STUDIO_CODE/whisper/DIMAURO/dati\"\n",
    "dimauro_reference_texts = {\n",
    "    'readtext': (\n",
    "        'Il ramarro della zia. Il papà (o il babbo come dice il piccolo Dado) era sul letto. '\n",
    "        'Sotto di lui, accanto al lago, sedeva Gigi, detto Ciccio, cocco della mamma e della nonna. '\n",
    "        'Vicino ad un sasso c’era una rosa rosso vivo e lo sciocco, vedendola, la volle per la zia. '\n",
    "        'La zia Lulù cercava zanzare per il suo ramarro, ma dato che era giugno (o luglio non so bene) '\n",
    "        'non ne trovava. Trovò invece una rana che saltando dalla strada finì nel lago con un grande spruzzo. '\n",
    "        'Sai che fifa, la zia! Lo schizzo bagnò il suo completo rosa che divenne giallo come un taxi. '\n",
    "        'Passava di lì un signore cosmopolita di nome Sardanapalo Nabucodonosor che si innamorò della zia '\n",
    "        'e la portò con sé in Afghanistan.'\n",
    "    ),\n",
    "    'words': (\n",
    "        'pipa, buco, topo, dado, casa, gatto, filo, vaso, muro, neve, luna, rete, zero, scia, '\n",
    "        'ciao, giro, sole, uomo, iuta, gnomo, glielo, pozzo, brodo, plagio, treno, classe, grigio, '\n",
    "        'flotta, creta, drago, frate, spesa, stufa, scala, slitta, splende, strada, scrive, spruzzo, '\n",
    "        'sgrido, sfregio, sdraio, sbrigo, prova, calendario, autobiografia, monotono, pericoloso, '\n",
    "        'montagnoso, prestigioso.'\n",
    "    ),\n",
    "    'phrases': (\n",
    "        'Oggi è una bella giornata per sciare. Voglio una maglia di lana color ocra. '\n",
    "        'Il motociclista attraversò una strada stretta di montagna. Patrizia ha pranzato a casa di Fabio. '\n",
    "        'Questo è il tuo cappello? Dopo vieni a casa? La televisione funziona? Non posso aiutarti? '\n",
    "        'Marco non è partito. Il medico non è impegnato.'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Calculate results for the DIMAURO dataset\n",
    "dimauro_results = calculate_wer_cer_for_groups(dimauro_directory, dimauro_data, dimauro_reference_texts, 'DIMAURO')\n",
    "\n",
    "# Define the paths and reference texts for the MOLINETTE dataset\n",
    "molinette_directory = r'/Users/benedettaperrone/Documents/TESI/VISUAL_STUDIO_CODE/whisper/MOLINETTE_combined/dati'\n",
    "\n",
    "# Calculate results for the MOLINETTE dataset\n",
    "molinette_results = calculate_wer_cer_for_groups(molinette_directory, molinette_data, {}, 'MOLINETTE')\n",
    "\n",
    "final_results = integrate_results(dimauro_results, molinette_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_and_save_results_as_table(dimauro_results, molinette_results, final_results, output_filename=\"summary_results_italiano.xlsx\"):\n",
    "    # Helper function to round values to 5 decimal places\n",
    "    def round_values(value):\n",
    "        if isinstance(value, (int, float)):\n",
    "            return round(value, 5)\n",
    "        return value\n",
    "\n",
    "    # Create DataFrames for DIMAURO and MOLINETTE results\n",
    "    dimauro_general_df = pd.DataFrame({\n",
    "        'Metric': ['Control WER', 'Parkinsonian WER', 'Control CER', 'Parkinsonian CER', 'Control Files Count', 'Parkinsonian Files Count'],\n",
    "        'Value': [round_values(dimauro_results['control_wer']), round_values(dimauro_results['parkinsonian_wer']), \n",
    "                  round_values(dimauro_results['control_cer']), round_values(dimauro_results['parkinsonian_cer']), \n",
    "                  dimauro_results['control_files_count'], dimauro_results['parkinsonian_files_count']]\n",
    "    })\n",
    "\n",
    "    molinette_general_df = pd.DataFrame({\n",
    "        'Metric': ['Control WER', 'Parkinsonian WER', 'Control CER', 'Parkinsonian CER', 'Control Files Count', 'Parkinsonian Files Count'],\n",
    "        'Value': [round_values(molinette_results['control_wer']), round_values(molinette_results['parkinsonian_wer']), \n",
    "                  round_values(molinette_results['control_cer']), round_values(molinette_results['parkinsonian_cer']), \n",
    "                  molinette_results['control_files_count'], molinette_results['parkinsonian_files_count']]\n",
    "    })\n",
    "\n",
    "    # Create DataFrames for DIMAURO results by UPDRS level\n",
    "    dimauro_updrs_df = pd.DataFrame({\n",
    "        'UPDRS Level': list(dimauro_results['updrs_files_count'].keys()),\n",
    "        'WER': [round_values(dimauro_results['parkinsonian_wer_by_updrs'][level]) for level in dimauro_results['updrs_files_count']],\n",
    "        'CER': [round_values(dimauro_results['parkinsonian_cer_by_updrs'][level]) for level in dimauro_results['updrs_files_count']],\n",
    "        'Files Count': [dimauro_results['updrs_files_count'][level] for level in dimauro_results['updrs_files_count']]\n",
    "    })\n",
    "\n",
    "    # Create DataFrame for MOLINETTE results by UPDRS level\n",
    "    molinette_updrs_df = pd.DataFrame({\n",
    "        'UPDRS Level': list(molinette_results['updrs_files_count'].keys()),\n",
    "        'WER': [round_values(molinette_results['parkinsonian_wer_by_updrs'][level]) for level in molinette_results['updrs_files_count']],\n",
    "        'CER': [round_values(molinette_results['parkinsonian_cer_by_updrs'][level]) for level in molinette_results['updrs_files_count']],\n",
    "        'Files Count': [molinette_results['updrs_files_count'][level] for level in molinette_results['updrs_files_count']]\n",
    "    })\n",
    "\n",
    "    # Create DataFrame for combined results\n",
    "    combined_results_df = pd.DataFrame({\n",
    "        'UPDRS Level': list(final_results['combined_wer'].keys()),\n",
    "        'WER Mean': [round_values(final_results['combined_wer'][level]['mean_unweighted']) for level in final_results['combined_wer']],\n",
    "        'WER Median': [round_values(final_results['combined_wer'][level]['median']) for level in final_results['combined_wer']],\n",
    "        'WER Stdev': [round_values(final_results['combined_wer'][level]['stdev']) for level in final_results['combined_wer']],\n",
    "        'CER Mean': [round_values(final_results['combined_cer'][level]['mean_unweighted']) for level in final_results['combined_cer']],\n",
    "        'CER Median': [round_values(final_results['combined_cer'][level]['median']) for level in final_results['combined_cer']],\n",
    "        'CER Stdev': [round_values(final_results['combined_cer'][level]['stdev']) for level in final_results['combined_cer']],\n",
    "        'Files Count': [final_results['total_files_by_updrs'][level] for level in final_results['combined_wer']]\n",
    "    })\n",
    "\n",
    "    # Create DataFrame for individual WER and CER values by UPDRS level\n",
    "    individual_values_df = pd.DataFrame({\n",
    "        'UPDRS Level': list(final_results['combined_wer'].keys()),\n",
    "        'Individual WER Values': [final_results['combined_wer'][level]['values'] for level in final_results['combined_wer']],\n",
    "        'Individual CER Values': [final_results['combined_cer'][level]['values'] for level in final_results['combined_cer']]\n",
    "    })\n",
    "\n",
    "    # Display the DataFrames\n",
    "    print(\"DIMAURO Results (General):\")\n",
    "    display(dimauro_general_df)\n",
    "    print(\"\\nDIMAURO Results by UPDRS Level:\")\n",
    "    display(dimauro_updrs_df)\n",
    "    print(\"\\nMOLINETTE Results (General):\")\n",
    "    display(molinette_general_df)\n",
    "    print(\"\\nMOLINETTE Results by UPDRS Level:\")\n",
    "    display(molinette_updrs_df)\n",
    "    print(\"\\nCombined Results:\")\n",
    "    display(combined_results_df)\n",
    "    print(\"\\nIndividual WER and CER Values by UPDRS Level:\")\n",
    "    display(individual_values_df)\n",
    "\n",
    "    # Save the DataFrames to a CSV file\n",
    "    with pd.ExcelWriter(output_filename) as writer:\n",
    "        dimauro_general_df.to_excel(writer, sheet_name='DIMAURO General', index=False)\n",
    "        dimauro_updrs_df.to_excel(writer, sheet_name='DIMAURO UPDRS', index=False)\n",
    "        molinette_general_df.to_excel(writer, sheet_name='MOLINETTE General', index=False)\n",
    "        molinette_updrs_df.to_excel(writer, sheet_name='MOLINETTE UPDRS', index=False)\n",
    "        combined_results_df.to_excel(writer, sheet_name='Combined Results', index=False)\n",
    "        individual_values_df.to_excel(writer, sheet_name='Individual Values', index=False)\n",
    "\n",
    "# Call the function to display the results as tables and save them to a CSV file\n",
    "display_and_save_results_as_table(dimauro_results, molinette_results, final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to remove outliers using Tukey's IQR method\n",
    "def remove_outliers_iqr(data):\n",
    "    \"\"\"Removes outliers using Tukey's IQR method and returns the clean data along with the list of outliers.\"\"\"\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Filter data into clean data and outliers\n",
    "    filtered_data = [x for x in data if lower_bound <= x <= upper_bound]\n",
    "    outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
    "    \n",
    "    return filtered_data, outliers\n",
    "\n",
    "# Function to process data and identify outliers for UPDRS levels 0, 1, and 2\n",
    "def prepare_data_and_outliers(data_by_updrs, metric_name):\n",
    "    \"\"\"\n",
    "    Prepares clean data and outliers for UPDRS levels 0, 1, and 2 by applying remove_outliers_iqr,\n",
    "    and prints information about the outliers.\n",
    "    \n",
    "    Args:\n",
    "        data_by_updrs (dict): Dictionary with UPDRS levels as keys and lists of data as values.\n",
    "        metric_name (str): Name of the metric (WER or CER).\n",
    "    \n",
    "    Returns:\n",
    "        clean_data_by_updrs (dict): Dictionary with UPDRS levels as keys and cleaned data as values.\n",
    "        outliers_by_updrs (dict): Dictionary with UPDRS levels as keys and outliers as values.\n",
    "    \"\"\"\n",
    "    clean_data_by_updrs = {}\n",
    "    outliers_by_updrs = {}\n",
    "    \n",
    "    for updrs_level in [0, 1, 2]:  # Only process UPDRS levels 0, 1, and 2\n",
    "        # Extract individual values for this UPDRS level\n",
    "        data = data_by_updrs.get(updrs_level, {}).get('values', [])\n",
    "        \n",
    "        # Process outliers\n",
    "        clean_data, outliers = remove_outliers_iqr(data)\n",
    "        clean_data_by_updrs[updrs_level] = clean_data\n",
    "        outliers_by_updrs[updrs_level] = outliers\n",
    "        \n",
    "        # Print information about outliers\n",
    "        print(f\"\\n### {metric_name} - UPDRS {updrs_level} ###\")\n",
    "        print(f\"Total Outliers: {len(outliers)}\")\n",
    "        print(\"Outlier Values:\", outliers)\n",
    "    \n",
    "    return clean_data_by_updrs, outliers_by_updrs\n",
    "\n",
    "# Function to plot boxplots with manual outliers for WER/CER data by UPDRS level\n",
    "def plot_boxplots_with_outliers(data_by_updrs, outliers_by_updrs, metric_name=\"WER\", dataset_label=\"Combined Bari and Molinette\"):\n",
    "    \"\"\"\n",
    "    Plots separate boxplots for UPDRS levels 0, 1, and 2, and manually adds outliers using scatter to ensure visibility.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare data for box plot using clean data for UPDRS levels 0, 1, and 2\n",
    "    data = [data_by_updrs[updrs] for updrs in [0, 1, 2]]\n",
    "    labels = [f'UPDRS {updrs}' for updrs in [0, 1, 2]]\n",
    "    \n",
    "    box = ax.boxplot(data, patch_artist=True, showmeans=True, meanline=True,\n",
    "                     labels=labels,\n",
    "                     widths=0.3,\n",
    "                     boxprops=dict(facecolor='lightblue', color='black'),\n",
    "                     medianprops=dict(color='red'),\n",
    "                     meanprops=dict(color='green', linewidth=2, marker=''),\n",
    "                     whiskerprops=dict(color='black'),\n",
    "                     capprops=dict(color='black'),\n",
    "                     flierprops=dict(marker='', markersize=0))  # Disable automatic outlier markers\n",
    "\n",
    "    # Add custom legend for Mean and Median\n",
    "    handles = [plt.Line2D([0], [0], color='green', label='Mean', linestyle='-'),\n",
    "               plt.Line2D([0], [0], color='red', label='Median', linestyle='-')]\n",
    "    ax.legend(handles=handles, loc='upper right', frameon=True, shadow=True, fontsize=12)\n",
    "    \n",
    "    # Plot outliers manually using scatter\n",
    "    for updrs_level, outliers in outliers_by_updrs.items():\n",
    "        if updrs_level in [0, 1, 2]:  # Only plot for UPDRS levels 0, 1, and 2\n",
    "            x_positions = np.full(len(outliers), [0, 1, 2].index(updrs_level) + 1)\n",
    "            ax.scatter(x_positions, outliers, color='black', marker='o', s=30)\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title(f'{metric_name} Distribution by UPDRS Levels ({dataset_label})', fontsize=18)\n",
    "    ax.set_ylabel(metric_name, fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Process and plot WER data\n",
    "clean_wer_by_updrs, wer_outliers_by_updrs = prepare_data_and_outliers(final_results['combined_wer'], \"WER\")\n",
    "plot_boxplots_with_outliers(clean_wer_by_updrs, wer_outliers_by_updrs, metric_name=\"WER\", dataset_label=\"Combined Bari and Molinette\")\n",
    "\n",
    "# Process and plot CER data\n",
    "clean_cer_by_updrs, cer_outliers_by_updrs = prepare_data_and_outliers(final_results['combined_cer'], \"CER\")\n",
    "plot_boxplots_with_outliers(clean_cer_by_updrs, cer_outliers_by_updrs, metric_name=\"CER\", dataset_label=\"Combined Bari and Molinette\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro, f_oneway, kruskal\n",
    "from scikit_posthocs import posthoc_dunn  # Ensure that scikit_posthocs is installed\n",
    "\n",
    "def perform_statistical_tests(parkinsonian_wer_by_updrs_clean, parkinsonian_cer_by_updrs_clean,\n",
    "                              output_filename=\"statistical_tests_results.csv\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    This function performs Shapiro-Wilk normality tests on WER and CER by UPDRS levels,\n",
    "    conducts significance testing with ANOVA or Kruskal-Wallis based on normality,\n",
    "    applies Dunn's post hoc test if needed, and saves all results to a CSV file.\n",
    "    Detailed debug information and interpretations for each test are provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Normality Tests\n",
    "    normality_results = {'Group': [], 'WER p-value': [], 'CER p-value': []}\n",
    "    for i in range(3):\n",
    "        normality_results['Group'].append(f'UPDRS {i}')\n",
    "        \n",
    "        # Shapiro-Wilk test for WER\n",
    "        if len(parkinsonian_wer_by_updrs_clean[i]) > 2:\n",
    "            wer_stat, wer_p_value = shapiro(parkinsonian_wer_by_updrs_clean[i])\n",
    "            normality_results['WER p-value'].append(wer_p_value)\n",
    "            print(f\"Debug: WER Shapiro-Wilk test for UPDRS {i} (p-value = {wer_p_value:.4f})\")\n",
    "        else:\n",
    "            normality_results['WER p-value'].append(float('nan'))\n",
    "        \n",
    "        # Shapiro-Wilk test for CER\n",
    "        if len(parkinsonian_cer_by_updrs_clean[i]) > 2:\n",
    "            cer_stat, cer_p_value = shapiro(parkinsonian_cer_by_updrs_clean[i])\n",
    "            normality_results['CER p-value'].append(cer_p_value)\n",
    "            print(f\"Debug: CER Shapiro-Wilk test for UPDRS {i} (p-value = {cer_p_value:.4f})\")\n",
    "        else:\n",
    "            normality_results['CER p-value'].append(float('nan'))\n",
    "\n",
    "    # Convert normality results to DataFrame and interpret them\n",
    "    normality_df = pd.DataFrame(normality_results)\n",
    "    print(\"\\n### Debug: Shapiro-Wilk Normality Test Results ###\")\n",
    "    print(normality_df.to_string(index=False))\n",
    "\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write(\"### Shapiro-Wilk Normality Test Results ###\\n\")\n",
    "        normality_df.to_csv(f, index=False)\n",
    "    \n",
    "    # Step 2: Significance Testing\n",
    "    significant_tests = {'Metric': [], 'Test': [], 'p-value': [], 'Conclusion': []}\n",
    "    dunn_results = []\n",
    "\n",
    "    # Test for WER\n",
    "    valid_wer_data = [parkinsonian_wer_by_updrs_clean[i] for i in range(3) if len(parkinsonian_wer_by_updrs_clean[i]) > 2]\n",
    "    if all(p_value > alpha for p_value in normality_df['WER p-value'] if not pd.isna(p_value)):\n",
    "        if len(valid_wer_data) > 1:\n",
    "            test_stat, p_value = f_oneway(*valid_wer_data)\n",
    "            test_name = \"ANOVA\"\n",
    "            test_description = \"ANOVA (data normally distributed)\"\n",
    "        else:\n",
    "            p_value = float('nan')\n",
    "            test_name = \"ANOVA (Insufficient data)\"\n",
    "            test_description = test_name\n",
    "    else:\n",
    "        if len(valid_wer_data) > 1:\n",
    "            test_stat, p_value = kruskal(*valid_wer_data)\n",
    "            test_name = \"Kruskal-Wallis\"\n",
    "            test_description = \"Kruskal-Wallis (data not normally distributed)\"\n",
    "            if p_value < alpha:\n",
    "                dunn_test_results = posthoc_dunn(valid_wer_data)\n",
    "                dunn_results.append(('WER', dunn_test_results))\n",
    "        else:\n",
    "            p_value = float('nan')\n",
    "            test_name = \"Kruskal-Wallis (Insufficient data)\"\n",
    "            test_description = test_name\n",
    "    \n",
    "    # Log WER significance test\n",
    "    significant_tests['Metric'].append('WER')\n",
    "    significant_tests['Test'].append(test_description)\n",
    "    significant_tests['p-value'].append(p_value)\n",
    "    conclusion = f\"The difference is {'statistically significant' if p_value < alpha else 'not statistically significant'} at alpha = {alpha}.\" if not pd.isna(p_value) else \"Insufficient data for valid test.\"\n",
    "    significant_tests['Conclusion'].append(conclusion)\n",
    "    print(f\"Debug: WER significance test - {conclusion}\")\n",
    "\n",
    "    # Test for CER\n",
    "    valid_cer_data = [parkinsonian_cer_by_updrs_clean[i] for i in range(3) if len(parkinsonian_cer_by_updrs_clean[i]) > 2]\n",
    "    if all(p_value > alpha for p_value in normality_df['CER p-value'] if not pd.isna(p_value)):\n",
    "        if len(valid_cer_data) > 1:\n",
    "            test_stat, p_value = f_oneway(*valid_cer_data)\n",
    "            test_name = \"ANOVA\"\n",
    "            test_description = \"ANOVA (data normally distributed)\"\n",
    "        else:\n",
    "            p_value = float('nan')\n",
    "            test_name = \"ANOVA (Insufficient data)\"\n",
    "            test_description = test_name\n",
    "    else:\n",
    "        if len(valid_cer_data) > 1:\n",
    "            test_stat, p_value = kruskal(*valid_cer_data)\n",
    "            test_name = \"Kruskal-Wallis\"\n",
    "            test_description = \"Kruskal-Wallis (data not normally distributed)\"\n",
    "            if p_value < alpha:\n",
    "                dunn_test_results = posthoc_dunn(valid_cer_data)\n",
    "                dunn_results.append(('CER', dunn_test_results))\n",
    "        else:\n",
    "            p_value = float('nan')\n",
    "            test_name = \"Kruskal-Wallis (Insufficient data)\"\n",
    "            test_description = test_name\n",
    "    \n",
    "    # Log CER significance test\n",
    "    significant_tests['Metric'].append('CER')\n",
    "    significant_tests['Test'].append(test_description)\n",
    "    significant_tests['p-value'].append(p_value)\n",
    "    conclusion = f\"The difference is {'statistically significant' if p_value < alpha else 'not statistically significant'} at alpha = {alpha}.\" if not pd.isna(p_value) else \"Insufficient data for valid test.\"\n",
    "    significant_tests['Conclusion'].append(conclusion)\n",
    "    print(f\"Debug: CER significance test - {conclusion}\")\n",
    "\n",
    "    # Convert to DataFrame for significance test results\n",
    "    tests_df = pd.DataFrame(significant_tests)\n",
    "    print(\"\\n### Debug: Significance Test Results ###\")\n",
    "    print(tests_df.to_string(index=False))\n",
    "    \n",
    "    # Save significance test and Dunn's test results to CSV\n",
    "    with open(output_filename, 'a') as f:\n",
    "        f.write(\"\\n\\n### Significance Test Results ###\\n\")\n",
    "        tests_df.to_csv(f, index=False)\n",
    "        \n",
    "        # Save Dunn's post hoc test results if Kruskal-Wallis was significant\n",
    "        for metric, result in dunn_results:\n",
    "            f.write(f\"\\n\\n### Dunn's Post Hoc Test Results for {metric} ###\\n\")\n",
    "            result.to_csv(f)\n",
    "\n",
    "    # Interpret and print Dunn’s post hoc test results\n",
    "    for metric, result in dunn_results:\n",
    "        print(f\"\\n### Dunn's Post Hoc Test Results for {metric} ###\")\n",
    "        print(result)\n",
    "        \n",
    "        print(\"\\nInterpretation of Dunn's Test Results (Significant Differences):\")\n",
    "        for (index, p_value) in result.stack().items():\n",
    "            level1, level2 = index\n",
    "            if p_value < alpha:\n",
    "                print(f\"UPDRS {level1} vs. UPDRS {level2}: Statistically significant difference (p-value = {p_value:.4f})\")\n",
    "            else:\n",
    "                print(f\"UPDRS {level1} vs. UPDRS {level2}: No statistically significant difference (p-value = {p_value:.4f})\")\n",
    "\n",
    "    print(\"\\n### Debug: All Results Saved to CSV ###\")\n",
    "    \n",
    "    return normality_df, tests_df, dunn_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the statistical test function with the cleaned WER and CER data\n",
    "normality_df, tests_df, dunn_results = perform_statistical_tests(\n",
    "    parkinsonian_wer_by_updrs_clean=clean_wer_by_updrs,\n",
    "    parkinsonian_cer_by_updrs_clean=clean_cer_by_updrs,\n",
    "    output_filename=\"summary_statistics_and_tests_combined.csv\",\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "# Print the Shapiro-Wilk normality test results with increased precision\n",
    "print(\"\\n### Shapiro-Wilk Normality Test Results ###\")\n",
    "pd.options.display.float_format = '{:.6f}'.format  # Set float format for higher precision\n",
    "print(normality_df.to_string(index=False))\n",
    "\n",
    "# Print the significance test results (ANOVA/Kruskal-Wallis) with increased precision\n",
    "print(\"\\n### Significance Test Results ###\")\n",
    "print(tests_df.to_string(index=False))\n",
    "\n",
    "# Display Dunn's Post Hoc Test results if any Kruskal-Wallis tests were significant\n",
    "if dunn_results:\n",
    "    for metric, result in dunn_results:\n",
    "        print(f\"\\n### Dunn's Post Hoc Test Results for {metric} ###\")\n",
    "        print(result)\n",
    "\n",
    "        # Print interpretation of Dunn's test results for significant differences with increased precision\n",
    "        print(\"\\nInterpretation of Dunn's Test Results (Significant Differences):\")\n",
    "        for (index, p_value) in result.stack().items():\n",
    "            level1, level2 = index\n",
    "            if p_value < 0.05:  # Adjusted p-value for Dunn's test\n",
    "                print(f\"UPDRS {level1} vs. UPDRS {level2}: Statistically significant difference (p-value = {p_value:.6f})\")\n",
    "            else:\n",
    "                print(f\"UPDRS {level1} vs. UPDRS {level2}: No statistically significant difference (p-value = {p_value:.6f})\")\n",
    "else:\n",
    "    print(\"\\nNo statistically significant differences found in Dunn's post hoc test.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract means and standard deviations\n",
    "wer_means = [final_results['combined_wer'][i]['mean_unweighted'] for i in range(3)]\n",
    "wer_stds = [np.std(final_results['combined_wer'][i]['values']) for i in range(3)]\n",
    "cer_means = [final_results['combined_cer'][i]['mean_unweighted'] for i in range(3)]\n",
    "cer_stds = [np.std(final_results['combined_cer'][i]['values']) for i in range(3)]\n",
    "\n",
    "# Define UPDRS groups and x positions\n",
    "updrs_groups = ['UPDRS 0', 'UPDRS 1', 'UPDRS 2']\n",
    "x_positions = np.arange(len(updrs_groups))\n",
    "\n",
    "# Function to add significance annotations\n",
    "def add_significance(ax, x1, x2, y, p_value, offset=0.02):\n",
    "    if p_value < 0.0001:\n",
    "        symbol = '****'\n",
    "    elif p_value < 0.001:\n",
    "        symbol = '***'\n",
    "    elif p_value < 0.01:\n",
    "        symbol = '**'\n",
    "    elif p_value < 0.05:\n",
    "        symbol = '*'\n",
    "    else:\n",
    "        return\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + offset, y + offset, y], color='black', linewidth=1.5)\n",
    "    ax.text((x1 + x2) * 0.5, y + offset + 0.01, symbol,\n",
    "            ha='center', va='bottom', color='black', fontsize=18)\n",
    "\n",
    "# Colors for bars\n",
    "bar_colors = ['lightgray', 'silver', 'darkgray']\n",
    "\n",
    "# Height offsets for significance annotations\n",
    "height_offsets = {(0, 1): 0.08, (0, 2): 0.20, (1, 2): 0.32}\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 16))\n",
    "\n",
    "# WER Plot\n",
    "ax = axes[0]\n",
    "ax.bar(x_positions, wer_means, yerr=wer_stds, capsize=5, color=bar_colors, alpha=0.8)\n",
    "ax.set_title('WER by UPDRS Group (Combined Bari and Molinette)', fontsize=24)\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(updrs_groups, fontsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylabel('WER (Mean ± SD)', fontsize=22)\n",
    "\n",
    "# Add WER significance\n",
    "wer_significance = dunn_results[0][1]\n",
    "for (i, j) in height_offsets.keys():\n",
    "    p_value = wer_significance.iloc[i, j]\n",
    "    y = max(wer_means[i] + wer_stds[i], wer_means[j] + wer_stds[j]) + height_offsets[(i, j)]\n",
    "    add_significance(ax, i, j, y, p_value, offset=0.02)\n",
    "\n",
    "# CER Plot\n",
    "ax = axes[1]\n",
    "ax.bar(x_positions, cer_means, yerr=cer_stds, capsize=5, color=bar_colors, alpha=0.8)\n",
    "ax.set_title('CER by UPDRS Group (Combined Bari and Molinette)', fontsize=24)\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(updrs_groups, fontsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.set_ylabel('CER (Mean ± SD)', fontsize=22)\n",
    "\n",
    "# Add CER significance\n",
    "cer_significance = dunn_results[1][1]\n",
    "for (i, j) in height_offsets.keys():\n",
    "    p_value = cer_significance.iloc[i, j]\n",
    "    y = max(cer_means[i] + cer_stds[i], cer_means[j] + cer_stds[j]) + height_offsets[(i, j)]\n",
    "    add_significance(ax, i, j, y, p_value, offset=0.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
