{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95084,
     "status": "ok",
     "timestamp": 1731668698023,
     "user": {
      "displayName": "Benedetta Perrone",
      "userId": "00214980196106492209"
     },
     "user_tz": -60
    },
    "id": "-ckLl2T5XkSW",
    "outputId": "d18bdeb6-94d9-45b7-f9bc-da7d2d3b9319"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "\n",
    "# Set the theme for seaborn plots\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
    "\n",
    "# Mount Google Drive to access files (if you're using Google Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change the working directory to the thesis folder in Google Drive\n",
    "thesis_path = #path\n",
    "os.chdir(thesis_path)\n",
    "\n",
    "# Verify the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Function to correct audio paths in the JSON data\n",
    "def correct_paths(data, base_path):\n",
    "    corrected_data = []\n",
    "    for item in data:\n",
    "        if 'audio' in item:\n",
    "            # Replace backslashes with slashes for compatibility\n",
    "            corrected_audio_path = item['audio'].replace('\\\\', '/')\n",
    "            # Ensure the path starts with the base path\n",
    "            if not corrected_audio_path.startswith(base_path):\n",
    "                corrected_audio_path = os.path.join(base_path, corrected_audio_path)\n",
    "            # Update the item's audio path\n",
    "            item['audio'] = corrected_audio_path\n",
    "        corrected_data.append(item)\n",
    "    return corrected_data\n",
    "\n",
    "# Function to process datasets\n",
    "def process_dataset(json_path, base_path, task_filter, dataset_name):\n",
    "    with open(json_path) as file:\n",
    "        data = json.load(file)\n",
    "    print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "    print(f\"Total items in the {dataset_name} dataset: {len(data)}\")\n",
    "\n",
    "    # Debug: Check the keys in the first few entries\n",
    "    print(f\"Keys in the first item: {data[0].keys()}\")\n",
    "\n",
    "    # Correct paths\n",
    "    data = correct_paths(data, base_path)\n",
    "\n",
    "    # Filter by task\n",
    "    filtered_data = [item for item in data if item['task'] in task_filter]\n",
    "    print(f\"Number of items found with tasks {task_filter}: {len(filtered_data)}\")\n",
    "\n",
    "    # Debug: Check the first few filtered items\n",
    "    for i, item in enumerate(filtered_data[:5]):\n",
    "        print(f\"Filtered item {i+1}: {item}\")\n",
    "\n",
    "    # Preprocess audio files\n",
    "    audio_lengths_before = []\n",
    "    audio_lengths_after = []\n",
    "    for idx, item in enumerate(filtered_data):\n",
    "        audio_path = item['audio']\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=44100)\n",
    "            length_before = len(audio) / sr\n",
    "            audio_lengths_before.append(length_before)\n",
    "\n",
    "            # Trim silence\n",
    "            audio_trimmed, _ = librosa.effects.trim(audio, top_db=20)\n",
    "            length_after = len(audio_trimmed) / sr\n",
    "            audio_lengths_after.append(length_after)\n",
    "\n",
    "            # Update the dataset with trimmed audio\n",
    "            item['audio'] = audio_trimmed\n",
    "\n",
    "            # Debug every 10 files\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(filtered_data)} files from {dataset_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {audio_path}: {e}\")\n",
    "\n",
    "    print(f\"Number of audio files successfully processed in {dataset_name}: {len(audio_lengths_after)}\")\n",
    "    return filtered_data, audio_lengths_before, audio_lengths_after\n",
    "\n",
    "# Process each dataset\n",
    "newdata = []\n",
    "\n",
    "pcgita_data, pcgita_lengths_before, pcgita_lengths_after = process_dataset(\n",
    "    'PCGITA.json', 'DATI', ['vowel','vowels'], 'PC-GITA'\n",
    ")\n",
    "newdata.extend(pcgita_data)\n",
    "\n",
    "BARI_data, BARI_lengths_before, BARI_lengths_after = process_dataset(\n",
    "    'BARI_new.json', 'DATI', ['vowels', 'vowelA'], 'BARI'\n",
    ")\n",
    "newdata.extend(BARI_data)\n",
    "\n",
    "molinette_data, molinette_lengths_before, molinette_lengths_after = process_dataset(\n",
    "    'MOLINETTE_combined.json', 'DATI', ['vowel', 'vowels'], 'MOLINETTE'\n",
    ")\n",
    "newdata.extend(molinette_data)\n",
    "\n",
    "# Debug: Check final dataset size and first few items\n",
    "print(f\"\\nTotal size of the combined dataset: {len(newdata)}\")\n",
    "print(\"First few items in the combined dataset:\")\n",
    "for i, item in enumerate(newdata[:5]):\n",
    "    print(f\"Item {i+1}: {item}\")\n",
    "\n",
    "# Save combined dataset to pickle file\n",
    "file_path_updated = '/content/drive/MyDrive/thesis/newdata_updated.pkl'\n",
    "with open(file_path_updated, 'wb') as f:\n",
    "    pickle.dump(newdata, f)\n",
    "\n",
    "print(f\"\\nCombined dataset successfully saved to {file_path_updated}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2253,
     "status": "ok",
     "timestamp": 1731668700274,
     "user": {
      "displayName": "Benedetta Perrone",
      "userId": "00214980196106492209"
     },
     "user_tz": -60
    },
    "id": "kOYmfcaGXoME",
    "outputId": "7ef44be5-d990-44df-c195-3c457662ba7d"
   },
   "outputs": [],
   "source": [
    "# Plot histograms for each dataset\n",
    "datasets = [\n",
    "    ('PC-GITA', pcgita_lengths_before, pcgita_lengths_after),\n",
    "    ('BARI', BARI_lengths_before, BARI_lengths_after),\n",
    "    ('MOLINETTE', molinette_lengths_before, molinette_lengths_after)\n",
    "]\n",
    "\n",
    "for name, lengths_before, lengths_after in datasets:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(lengths_before, bins=30, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Audio Length (seconds)')\n",
    "    plt.ylabel('Number of Audio Files')\n",
    "    plt.title(f'Distribution of Audio Lengths Before Trimming ({name})')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(lengths_after, bins=30, alpha=0.7, color='red')\n",
    "    plt.xlabel('Audio Length (seconds)')\n",
    "    plt.ylabel('Number of Audio Files')\n",
    "    plt.title(f'Distribution of Audio Lengths After Trimming ({name})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2189,
     "status": "ok",
     "timestamp": 1731668702461,
     "user": {
      "displayName": "Benedetta Perrone",
      "userId": "00214980196106492209"
     },
     "user_tz": -60
    },
    "id": "JdbNY-6QYI3a",
    "outputId": "4cdaa0b0-b108-4a40-e205-961f0f56c21d"
   },
   "outputs": [],
   "source": [
    "# Path to the .pkl file\n",
    "file_path = 'path/to/your/data.pkl'\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(file_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "# Total number of elements in the dataset\n",
    "print(f\"Total number of elements in dataset: {len(dataset)}\")\n",
    "\n",
    "# Inspect keys available in the first element\n",
    "print(\"\\nKeys in the first element:\")\n",
    "print(dataset[0].keys())\n",
    "\n",
    "# Display the first 5 elements\n",
    "print(\"\\nFirst 5 elements in dataset:\")\n",
    "for i, item in enumerate(dataset[:5]):\n",
    "    print(f\"Element {i + 1}: {item}\")\n",
    "\n",
    "# Statistics for some keys\n",
    "if 'audio' in dataset[0]:\n",
    "    print(\"\\nChecking 'audio' field:\")\n",
    "    print(f\"First audio path: {dataset[0]['audio']}\")\n",
    "\n",
    "if 'task' in dataset[0]:\n",
    "    print(\"\\nTask distribution:\")\n",
    "    tasks = [item['task'] for item in dataset]\n",
    "    print({task: tasks.count(task) for task in set(tasks)})\n",
    "\n",
    "if 'updrs' in dataset[0]:\n",
    "    print(\"\\nUPDRS distribution (if available):\")\n",
    "    updrs_levels = [item['updrs'] for item in dataset if 'updrs' in item]\n",
    "    print({level: updrs_levels.count(level) for level in set(updrs_levels)})\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMEOyYvP0JcqC5U3f75BzSY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
