{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 10233,
     "status": "ok",
     "timestamp": 1732705351167,
     "user": {
      "displayName": "Benedetta Perrone",
      "userId": "00214980196106492209"
     },
     "user_tz": -60
    },
    "id": "R-i5JMXpNsoR",
    "outputId": "5c3135fc-8b12-4694-cc44-4421a86510ae"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, balanced_accuracy_score, confusion_matrix\n",
    ")\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification\n",
    "from collections import defaultdict\n",
    "from google.colab import drive\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "tesi_path = '/content/drive/My Drive/TESI'\n",
    "os.chdir(tesi_path)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "file_path = '/content/drive/MyDrive/TESI/newdata_updated.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    newdata = pickle.load(f)\n",
    "\n",
    "print(\"Pickle file loaded successfully.\")\n",
    "print(f\"Total samples in dataset: {len(newdata)}\")\n",
    "\n",
    "sampling_rate = 44100\n",
    "\n",
    "audio_lengths_sec = [len(item['audio']) / sampling_rate for item in newdata]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(audio_lengths_sec, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Audio Lengths (Before Processing)')\n",
    "plt.xlabel('Audio Length (seconds)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of audios: {len(audio_lengths_sec)}\")\n",
    "print(f\"Minimum audio length: {min(audio_lengths_sec):.2f} seconds\")\n",
    "print(f\"Maximum audio length: {max(audio_lengths_sec):.2f} seconds\")\n",
    "\n",
    "def print_updrs_distribution(data, label_key='label', updrs_keys=['updrs', 'UPDRS']):\n",
    "    \"\"\"\n",
    "    Prints the distribution of data based on UPDRS levels and labels.\n",
    "    \"\"\"\n",
    "    updrs_counts = defaultdict(int)\n",
    "    control_count, parkinsonian_count = 0, 0\n",
    "\n",
    "    for item in data:\n",
    "        if label_key in item:\n",
    "            label = item[label_key]\n",
    "            if label == 0:\n",
    "                control_count += 1\n",
    "            elif label == 1:\n",
    "                parkinsonian_count += 1\n",
    "\n",
    "        updrs_value = None\n",
    "        for key in updrs_keys:\n",
    "            if key in item:\n",
    "                updrs_value = item[key]\n",
    "                break\n",
    "\n",
    "        if updrs_value is not None:\n",
    "            updrs_counts[updrs_value] += 1\n",
    "\n",
    "    print(f\"Number of controls: {control_count}\")\n",
    "    print(f\"Number of Parkinsonians: {parkinsonian_count}\")\n",
    "    print(\"UPDRS Distribution:\")\n",
    "    for updrs_value, count in sorted(updrs_counts.items()):\n",
    "        print(f\"  UPDRS {updrs_value}: {count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1732705351874,
     "user": {
      "displayName": "Benedetta Perrone",
      "userId": "00214980196106492209"
     },
     "user_tz": -60
    },
    "id": "b0fQJqUn1a9J",
    "outputId": "97b6193f-d2c9-4f91-a208-d3fdf1135ea1"
   },
   "outputs": [],
   "source": [
    "# Remove outliers based on audio length using IQR\n",
    "q1 = np.percentile(audio_lengths, 25)\n",
    "q3 = np.percentile(audio_lengths, 75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "filtered_data = [item for item in newdata if lower_bound <= len(item['audio']) <= upper_bound]\n",
    "filtered_audio_lengths_sec = [len(item['audio']) / sampling_rate for item in filtered_data]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(filtered_audio_lengths_sec, bins=50, color='green', alpha=0.7)\n",
    "plt.title('Distribution of Audio Lengths (After Removing Outliers)')\n",
    "plt.xlabel('Audio Length (seconds)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of audios after removing outliers: {len(filtered_audio_lengths_sec)}\")\n",
    "print(f\"Min audio length after filtering: {min(filtered_audio_lengths_sec):.2f} seconds\")\n",
    "print(f\"Max audio length after filtering: {max(filtered_audio_lengths_sec):.2f} seconds\")\n",
    "\n",
    "min_length = min([len(item['audio']) for item in filtered_data])\n",
    "print(f\"Shortest audio length: {min_length / sampling_rate:.2f} seconds ({min_length} samples)\")\n",
    "\n",
    "print(\"Distribution BEFORE outlier removal:\")\n",
    "print_updrs_distribution(newdata)\n",
    "print(\"Distribution AFTER outlier removal:\")\n",
    "print_updrs_distribution(filtered_data)\n",
    "\n",
    "newdata = filtered_data\n",
    "\n",
    "min_length = min([len(item['audio']) for item in newdata])\n",
    "for item in newdata:\n",
    "    start = (len(item['audio']) - min_length) // 2\n",
    "    end = start + min_length\n",
    "    item['audio'] = item['audio'][start:end]\n",
    "\n",
    "for item in newdata:\n",
    "    audio = np.array(item['audio'])\n",
    "    item['audio'] = audio / np.max(np.abs(audio))\n",
    "\n",
    "print(f\"Example audio shape: {newdata[0]['audio'].shape}\")\n",
    "\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(\n",
    "    'MIT/ast-finetuned-audioset-10-10-0.4593',\n",
    "    sampling_rate=16000,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, extractor, dropout_rate=0.15):\n",
    "        self.data = data\n",
    "        self.extractor = extractor\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        audio = sample['audio']\n",
    "        label = sample['label']\n",
    "\n",
    "        updrs_value = sample.get('updrs', sample.get('UPDRS', -1))\n",
    "        metadata = {'updrs': updrs_value}\n",
    "\n",
    "        inputs = self.extractor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        input_values = self.dropout(inputs['input_values'].squeeze(0))\n",
    "\n",
    "        return input_values, label, metadata\n",
    "\n",
    "def stratified_group_split(all_ids, grouped_by_id, label_key='label'):\n",
    "    controls = [id_ for id_ in all_ids if grouped_by_id[id_][0][label_key] == 0]\n",
    "    parkinsons = [id_ for id_ in all_ids if grouped_by_id[id_][0][label_key] == 1]\n",
    "\n",
    "    random.shuffle(controls)\n",
    "    random.shuffle(parkinsons)\n",
    "\n",
    "    split_controls = len(controls) // 5\n",
    "    split_parkinsons = len(parkinsons) // 5\n",
    "\n",
    "    folds = []\n",
    "    for i in range(5):\n",
    "        fold_controls = controls[i * split_controls:(i + 1) * split_controls]\n",
    "        fold_parkinsons = parkinsons[i * split_parkinsons:(i + 1) * split_parkinsons]\n",
    "        folds.append(fold_controls + fold_parkinsons)\n",
    "\n",
    "    return folds\n",
    "\n",
    "def analyze_updrs(val_loader, model, device):\n",
    "    updrs_results = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, metadata in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).logits\n",
    "\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            logits = outputs[:, 1]\n",
    "\n",
    "            for i, prob in enumerate(probabilities.cpu().numpy()):\n",
    "                logit = logits[i].item()\n",
    "                updrs_value = metadata.get('updrs', None)\n",
    "                if updrs_value is None:\n",
    "                    continue\n",
    "                if isinstance(updrs_value, torch.Tensor):\n",
    "                    updrs_value = updrs_value[i].item()\n",
    "                else:\n",
    "                    updrs_value = int(updrs_value[i])\n",
    "\n",
    "                if updrs_value == -1:\n",
    "                    continue\n",
    "\n",
    "                updrs_results[updrs_value].append((logit, prob, labels[i].item()))\n",
    "\n",
    "    return updrs_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_updrs_results(fold_results):\n",
    "    \"\"\"\n",
    "    Aggregates UPDRS results across all folds.\n",
    "    \"\"\"\n",
    "    aggregated_results = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "    probabilities_by_updrs = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "\n",
    "    for fold in fold_results:\n",
    "        updrs_results = fold['updrs_results']\n",
    "        for level, values in updrs_results.items():\n",
    "            aggregated_results[level].extend(values)\n",
    "            probabilities_by_updrs[level].extend([prob for _, prob, _ in values])\n",
    "\n",
    "    metrics = {}\n",
    "    for level, results in aggregated_results.items():\n",
    "        if results:\n",
    "            logits, probs, true_labels = zip(*results)\n",
    "            mean_prob = np.mean(probs)\n",
    "            mean_logit = np.mean(logits)\n",
    "            total_count = len(results)\n",
    "            classified_as_parkinsonian = sum(1 for prob, label in zip(probs, true_labels) if prob >= 0.5 and label == 1)\n",
    "            percentage_classified_as_parkinsonian = (classified_as_parkinsonian / total_count) * 100\n",
    "\n",
    "            metrics[level] = {\n",
    "                'total_count': total_count,\n",
    "                'mean_probability': mean_prob,\n",
    "                'mean_logit': mean_logit,\n",
    "                'percentage_classified_as_parkinsonian': percentage_classified_as_parkinsonian,\n",
    "            }\n",
    "        else:\n",
    "            metrics[level] = {\n",
    "                'total_count': 0,\n",
    "                'mean_probability': 0.0,\n",
    "                'mean_logit': 0.0,\n",
    "                'percentage_classified_as_parkinsonian': 0.0,\n",
    "            }\n",
    "\n",
    "    return metrics, probabilities_by_updrs\n",
    "\n",
    "\n",
    "def compute_confusion_matrix_metrics(labels, preds, fold_num):\n",
    "    \"\"\"\n",
    "    Computes confusion matrix, sensitivity, specificity, and balanced accuracy.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    balanced_acc = (sensitivity + specificity) / 2\n",
    "\n",
    "    print(f\"\\nConfusion Matrix for Fold {fold_num}:\")\n",
    "    print(cm)\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "    return sensitivity, specificity, balanced_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlJAELEkN1J8",
    "outputId": "4d4ff676-700f-437c-b28a-18b583afa94c"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 5\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.995\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Group data by subject ID\n",
    "grouped_by_id = defaultdict(list)\n",
    "for item in newdata:\n",
    "    grouped_by_id[item['id']].append(item)\n",
    "\n",
    "all_ids = list(grouped_by_id.keys())\n",
    "folds = stratified_group_split(all_ids, grouped_by_id)\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"Processing Fold {fold+1}/5...\")\n",
    "\n",
    "    train_ids = [id_ for id_ in all_ids if id_ not in folds[fold]]\n",
    "    val_ids = folds[fold]\n",
    "\n",
    "    train_samples = [item for id_s in train_ids for item in grouped_by_id[id_s]]\n",
    "    val_samples = [item for id_s in val_ids for item in grouped_by_id[id_s]]\n",
    "\n",
    "    print(f\"Training set size: {len(train_samples)}, Validation set size: {len(val_samples)}\")\n",
    "\n",
    "    train_dataset = AudioDataset(train_samples, feature_extractor)\n",
    "    val_dataset = AudioDataset(val_samples, feature_extractor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = ASTForAudioClassification.from_pretrained(\n",
    "        'MIT/ast-finetuned-audioset-10-10-0.4593',\n",
    "        num_labels=2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    # Freeze all layers except last 2 encoder layers and classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'encoder.layer.11' in name or 'encoder.layer.10' in name or 'classifier' in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    epoch_train_accuracies, epoch_val_accuracies = [], []\n",
    "    epoch_train_precisions, epoch_val_precisions = [], []\n",
    "    epoch_train_recalls, epoch_val_recalls = [], []\n",
    "    epoch_train_f1s, epoch_val_f1s = [], []\n",
    "    epoch_train_losses, epoch_val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} for Fold {fold+1}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, correct_train = 0.0, 0\n",
    "        train_preds, train_true = [], []\n",
    "\n",
    "        for inputs, labels, metadata in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * labels.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_train += torch.sum(preds == labels).item()\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "        train_precision = precision_score(train_true, train_preds, zero_division=0)\n",
    "        train_recall = recall_score(train_true, train_preds)\n",
    "        train_f1 = f1_score(train_true, train_preds)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct_val = 0.0, 0\n",
    "        val_preds, val_true = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, metadata in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct_val += torch.sum(preds == labels).item()\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "        val_precision = precision_score(val_true, val_preds, zero_division=0)\n",
    "        val_recall = recall_score(val_true, val_preds)\n",
    "        val_f1 = f1_score(val_true, val_preds)\n",
    "\n",
    "        true_negatives = np.sum((np.array(val_true) == 0) & (np.array(val_preds) == 0))\n",
    "        false_positives = np.sum((np.array(val_true) == 0) & (np.array(val_preds) == 1))\n",
    "        val_specificity = true_negatives / (true_negatives + false_positives)\n",
    "        val_sensitivity = val_recall\n",
    "\n",
    "        updrs_results = analyze_updrs(val_loader, model, device)\n",
    "\n",
    "        # Store epoch metrics\n",
    "        epoch_train_accuracies.append(train_accuracy)\n",
    "        epoch_val_accuracies.append(val_accuracy)\n",
    "        epoch_train_precisions.append(train_precision)\n",
    "        epoch_val_precisions.append(val_precision)\n",
    "        epoch_train_recalls.append(train_recall)\n",
    "        epoch_val_recalls.append(val_recall)\n",
    "        epoch_train_f1s.append(train_f1)\n",
    "        epoch_val_f1s.append(val_f1)\n",
    "        epoch_train_losses.append(train_loss)\n",
    "        epoch_val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1} - Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_fold_{fold+1}.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Store fold results\n",
    "    fold_results.append({\n",
    "        'train_accuracy': epoch_train_accuracies,\n",
    "        'val_accuracy': epoch_val_accuracies,\n",
    "        'train_precision': epoch_train_precisions,\n",
    "        'val_precision': epoch_val_precisions,\n",
    "        'train_recall': epoch_train_recalls,\n",
    "        'val_recall': epoch_val_recalls,\n",
    "        'train_f1': epoch_train_f1s,\n",
    "        'val_f1': epoch_val_f1s,\n",
    "        'train_loss': epoch_train_losses,\n",
    "        'val_loss': epoch_val_losses,\n",
    "        'val_sensitivity': val_sensitivity,\n",
    "        'val_specificity': val_specificity,\n",
    "        'val_preds': val_preds,\n",
    "        'val_true': val_true,\n",
    "        'updrs_results': updrs_results\n",
    "    })\n",
    "\n",
    "# Compute final averages (last epoch of each fold)\n",
    "average_train_accuracy = np.mean([r['train_accuracy'][-1] for r in fold_results])\n",
    "average_val_accuracy = np.mean([r['val_accuracy'][-1] for r in fold_results])\n",
    "average_train_precision = np.mean([r['train_precision'][-1] for r in fold_results])\n",
    "average_val_precision = np.mean([r['val_precision'][-1] for r in fold_results])\n",
    "average_train_recall = np.mean([r['train_recall'][-1] for r in fold_results])\n",
    "average_val_recall = np.mean([r['val_recall'][-1] for r in fold_results])\n",
    "average_train_f1 = np.mean([r['train_f1'][-1] for r in fold_results])\n",
    "average_val_f1 = np.mean([r['val_f1'][-1] for r in fold_results])\n",
    "average_val_sensitivity = np.mean([r['val_sensitivity'] for r in fold_results])\n",
    "average_val_specificity = np.mean([r['val_specificity'] for r in fold_results])\n",
    "\n",
    "print(\"\\n===== Average Metrics Across Folds (Last Epoch Only) =====\")\n",
    "print(f\"Training Accuracy: {average_train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {average_val_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {average_train_precision:.4f}\")\n",
    "print(f\"Validation Precision: {average_val_precision:.4f}\")\n",
    "print(f\"Training Recall: {average_train_recall:.4f}\")\n",
    "print(f\"Validation Recall: {average_val_recall:.4f}\")\n",
    "print(f\"Training F1-Score: {average_train_f1:.4f}\")\n",
    "print(f\"Validation F1-Score: {average_val_f1:.4f}\")\n",
    "print(f\"Validation Sensitivity: {average_val_sensitivity:.4f}\")\n",
    "print(f\"Validation Specificity: {average_val_specificity:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eRzCK23JaZY"
   },
   "outputs": [],
   "source": [
    "aggregated_updrs_metrics, probabilities_by_updrs = aggregate_updrs_results(fold_results)\n",
    "\n",
    "print(\"\\n===== Aggregated UPDRS Metrics Across Folds =====\")\n",
    "for level, metrics in aggregated_updrs_metrics.items():\n",
    "    print(f\"UPDRS Level {level}:\")\n",
    "    print(f\"  Total Count: {metrics['total_count']}\")\n",
    "    print(f\"  Mean Probability (Parkinsonian): {metrics['mean_probability']:.4f}\")\n",
    "    print(f\"  Percentage Classified as Parkinsonian: {metrics['percentage_classified_as_parkinsonian']:.2f}%\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwRqfs1lJppP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_updrs_metrics(aggregated_updrs_metrics, probabilities_by_updrs):\n",
    "    levels = list(aggregated_updrs_metrics.keys())\n",
    "    total_counts = [aggregated_updrs_metrics[level]['total_count'] for level in levels]\n",
    "    mean_probs = [aggregated_updrs_metrics[level]['mean_probability'] for level in levels]\n",
    "    mean_logits = [aggregated_updrs_metrics[level]['mean_logit'] for level in levels]\n",
    "    percentages_classified = [aggregated_updrs_metrics[level]['percentage_classified_as_parkinsonian'] for level in levels]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=levels, y=mean_probs, palette='Blues_d')\n",
    "    plt.xlabel('UPDRS Level')\n",
    "    plt.ylabel('Mean Probability')\n",
    "    plt.title('Mean Probability of Being Classified as Parkinsonian by UPDRS Level')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data = [(level, prob) for level, probs in probabilities_by_updrs.items() for prob in probs]\n",
    "    df = pd.DataFrame(data, columns=['UPDRS Level', 'Probability'])\n",
    "    sns.boxplot(x='UPDRS Level', y='Probability', data=df, palette='Pastel1')\n",
    "    plt.xlabel('UPDRS Level')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Distribution of Probabilities by UPDRS Level')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=levels, y=percentages_classified, palette='Oranges_d')\n",
    "    plt.xlabel('UPDRS Level')\n",
    "    plt.ylabel('Percentage Classified as Parkinsonian')\n",
    "    plt.title('Percentage Classified as Parkinsonian by UPDRS Level')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=levels, y=total_counts, palette='Greens_d')\n",
    "    plt.xlabel('UPDRS Level')\n",
    "    plt.ylabel('Total Count')\n",
    "    plt.title('Total Data Count by UPDRS Level')\n",
    "    plt.show()\n",
    "\n",
    "plot_updrs_metrics(aggregated_updrs_metrics, probabilities_by_updrs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
